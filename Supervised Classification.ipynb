{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09840ae-9a17-4f6b-a89f-0e32bfa9079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1 : What is Information Gain, and how is it used in Decision Trees?\n",
    "'''Information Gain (IG) is a measure used in Decision Trees to determine which feature best splits the data into classes. \n",
    "It quantifies the reduction in uncertainty or entropy achieved by splitting the data based on a particular feature. \n",
    "The higher the Information Gain, the more useful the feature is for classification. It is calculated as the difference between the entropy \n",
    "of the parent node and the weighted average entropy of the child nodes. In simple terms, it tells us how much ‚Äúinformation‚Äù a feature gives \n",
    "about the target variable. Decision Trees select the feature with the highest Information Gain at each split to build the model efficiently.\n",
    "In Decision Trees, Information Gain is used to decide which feature to split the data on at each node. For every feature, \n",
    "the tree calculates the Information Gain ‚Äî the reduction in entropy (or impurity) after splitting the dataset based on that feature. \n",
    "The feature with the highest Information Gain is chosen for the split because it best separates the data into pure subsets. \n",
    "This process is repeated recursively at each node until the tree perfectly classifies the data or meets a stopping condition. \n",
    "In short, Information Gain helps the tree grow in the most informative and efficient way.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7137375e-6579-4423-a308-fc77e5d835a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2: What is the difference between Gini Impurity and Entropy?\n",
    "#Hint: Directly compares the two main impurity measures, highlighting strengths, weaknesses, and appropriate use cases.\n",
    "'''Both Gini Impurity and Entropy are measures of impurity used in Decision Trees to determine how well a feature splits the data.\n",
    "\n",
    "| **Aspect**       | **Gini Impurity**                                                              | **Entropy**                                                     |\n",
    "| ---------------- | ------------------------------------------------------------------------------ | --------------------------------------------------------------- |\n",
    "| **Definition**   | Measures the probability of incorrectly classifying a randomly chosen element. | Measures the amount of disorder or randomness in the data.      |\n",
    "| **Formula**      | ( Gini = 1 - \\sum p_i^2 )                                                      | ( Entropy = -\\sum p_i \\log_2(p_i) )                             |\n",
    "| **Range**        | 0 (pure) to 0.5 (most impure for binary classes)                               | 0 (pure) to 1 (most impure for binary classes)                  |\n",
    "| **Computation**  | Simpler and faster to compute.                                                 | Slightly more complex due to logarithmic calculations.          |\n",
    "| **Tendency**     | Prefers larger partitions with dominant classes.                               | More sensitive to class distribution (penalizes impurity more). |\n",
    "| **Common Usage** | Default measure in **CART** (Classification and Regression Trees).             | Used in **ID3** and **C4.5** Decision Tree algorithms.          |\n",
    "\n",
    "In summary:\n",
    "Both aim to create pure nodes, but Gini Impurity is computationally efficient, while Entropy gives a more information-theoretic perspective. \n",
    "In practice, they often lead to similar results.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3a0b4c-e50c-4585-b070-935b8524a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3:What is Pre-Pruning in Decision Trees?\n",
    "'''Pre-pruning, also known as early stopping, is a technique used to stop the growth of a Decision Tree before it becomes too complex. \n",
    "Instead of allowing the tree to grow fully and then pruning it, pre-pruning applies certain conditions during the building process to decide \n",
    "when to stop splitting a node.\n",
    "\n",
    "Common stopping criteria include:\n",
    "\n",
    "* The node reaches a maximum depth.\n",
    "* The number of samples in a node is below a set threshold.\n",
    "* The Information Gain or Gini decrease from a split is too small.\n",
    "* The node becomes pure (all samples belong to one class).\n",
    "\n",
    "Pre-pruning helps prevent overfitting, reduces model complexity, and improves generalization by ensuring the tree doesn‚Äôt fit noise in the \n",
    "training data.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5924a97-57fe-499d-83b2-8de59f9e16ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances:\n",
      "sepal length (cm): 0.0000\n",
      "sepal width (cm): 0.0133\n",
      "petal length (cm): 0.0641\n",
      "petal width (cm): 0.9226\n"
     ]
    }
   ],
   "source": [
    "#Question 4:Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).\n",
    "#Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.\n",
    "# Import necessary libraries\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load sample dataset (Iris dataset)\n",
    "data = load_iris()\n",
    "X = data.data        # Features\n",
    "y = data.target      # Target labels\n",
    "\n",
    "# Create and train Decision Tree Classifier using Gini Impurity\n",
    "model = DecisionTreeClassifier(criterion='gini', random_state=0)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Print feature importances\n",
    "print(\"Feature Importances:\")\n",
    "for name, importance in zip(data.feature_names, model.feature_importances_):\n",
    "    print(f\"{name}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e7ef20-fd69-4e82-8cab-8f26693705c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 5: What is a Support Vector Machine (SVM)?\n",
    "'''A Support Vector Machine (SVM) is a powerful supervised learning algorithm used for classification and regression tasks. \n",
    "It works by finding the optimal hyperplane that best separates the data points of different classes in the feature space.\n",
    "The goal is to maximize the margin, which is the distance between the hyperplane and the nearest data points from each class, known as support vectors.\n",
    "\n",
    "SVM can also handle non-linear data using the kernel trick, which transforms data into a higher-dimensional space where it becomes linearly separable. \n",
    "It is widely used because of its high accuracy, robustness to overfitting, and effectiveness in high-dimensional spaces such as text classification\n",
    "and image recognition.\n",
    "\n",
    "üîπ Key Concepts:\n",
    "\n",
    "Hyperplane:\n",
    "\n",
    "A line (in 2D) or plane (in higher dimensions) that separates different classes of data.\n",
    "\n",
    "SVM tries to find the hyperplane that maximizes the margin ‚Äî the distance between the hyperplane and the closest data points from each class.\n",
    "\n",
    "Support Vectors:\n",
    "\n",
    "The data points closest to the hyperplane that influence its position and orientation.\n",
    "\n",
    "These are the most critical points in determining the decision boundary.\n",
    "\n",
    "Margin:\n",
    "\n",
    "The gap between the support vectors of different classes.\n",
    "\n",
    "A larger margin generally means better generalization and less overfitting.'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f759b4b-27e0-477b-af50-e0b61de8882a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 6: What is the Kernel Trick in SVM?\n",
    "'''The Kernel Trick is a mathematical technique used in Support Vector Machines (SVMs) to handle non-linearly separable data. Instead of explicitly\n",
    "transforming data into a higher-dimensional space, the kernel trick computes the inner products of data points in that higher-dimensional space \n",
    "without actually performing the transformation.\n",
    "\n",
    "This allows SVMs to create non-linear decision boundaries efficiently and with less computation. The idea is to apply a kernel function, \n",
    "which measures similarity between two data points, enabling the SVM to find complex boundaries in the original input space.\n",
    "\n",
    "Common kernel functions include:\n",
    "Linear Kernel: \n",
    "K(x,y)=x‚ãÖy\n",
    "\n",
    "Polynomial Kernel: \n",
    "K(x,y)=(x‚ãÖy+c)d\n",
    "\n",
    "RBF (Radial Basis Function) Kernel: \n",
    "K(x,y)=e‚àíŒ≥‚à£‚à£x‚àíy‚à£‚à£2\n",
    "In short, the kernel trick allows SVMs to efficiently solve problems where data cannot be separated by a straight line, by implicitly working\n",
    "in a higher-dimensional space.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02bf53c2-b98f-432c-937e-9f60b3d975d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Linear Kernel: 1.0000\n",
      "Accuracy with RBF Kernel: 0.8056\n"
     ]
    }
   ],
   "source": [
    "#Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
    "#Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting on the same dataset\n",
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = datasets.load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVM with Linear Kernel\n",
    "svm_linear = SVC(kernel='linear', random_state=42)\n",
    "svm_linear.fit(X_train, y_train)\n",
    "\n",
    "# Train SVM with RBF Kernel\n",
    "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_linear = svm_linear.predict(X_test)\n",
    "y_pred_rbf = svm_rbf.predict(X_test)\n",
    "\n",
    "# Calculate accuracies\n",
    "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
    "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy with Linear Kernel: {acc_linear:.4f}\")\n",
    "print(f\"Accuracy with RBF Kernel: {acc_rbf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d25e01-fa60-4169-a449-b5c84dbaa27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 8: What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"?\n",
    "'''The Na√Øve Bayes classifier is a probabilistic machine learning algorithm based on Bayes‚Äô Theorem, used mainly for classification tasks. \n",
    "It assumes that all features are independent of each other given the target class ‚Äî hence the term ‚Äúna√Øve.‚Äù Despite this simple assumption,\n",
    "it performs remarkably well in many real-world applications.\n",
    "\n",
    "The classifier calculates the posterior probability of each class given the input features and assigns the class with the highest probability. \n",
    "It is highly efficient, works well with large datasets, and is commonly used in text classification, spam detection, and sentiment analysis. \n",
    "Variants include Gaussian, Multinomial, and Bernoulli Na√Øve Bayes, depending on the data type.\n",
    "\n",
    "It is called ‚ÄúNa√Øve‚Äù because the algorithm makes a simplifying assumption that all features are independent of each other given the class label. \n",
    "In real-world data, features are often correlated (for example, the words ‚Äúbuy‚Äù and ‚Äúoffer‚Äù in spam emails), but Na√Øve Bayes ignores these dependencies\n",
    "to simplify computation. This assumption makes the model mathematically simple and computationally efficient. Despite being ‚Äúna√Øve,‚Äù it performs \n",
    "surprisingly well in many applications like spam detection and text classification, where exact independence isn‚Äôt crucial.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2694a674-a286-4506-a7df-86618c2181e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 9: Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√ØveBayes, and Bernoulli Na√Øve Bayes?\n",
    "'''\n",
    "All three are types of Na√Øve Bayes classifiers, differing mainly in the type of data they are designed to handle and how they model feature \n",
    "probabilities.\n",
    "\n",
    "| **Type**                    | **Data Type**               | **Assumption / Distribution**                                           | **Typical Use Case**                                                      |\n",
    "| --------------------------- | --------------------------- | ----------------------------------------------------------------------- | ------------------------------------------------------------------------- |\n",
    "| **Gaussian Na√Øve Bayes**    | Continuous (numerical) data | Assumes that features follow a **normal (Gaussian) distribution**       | Used for real-valued features like height, weight, or sensor readings     |\n",
    "| **Multinomial Na√Øve Bayes** | Discrete count data         | Assumes features represent **frequency counts** (non-negative integers) | Commonly used in **text classification** (e.g., word counts in documents) |\n",
    "| **Bernoulli Na√Øve Bayes**   | Binary data (0s and 1s)     | Assumes features are **Boolean** ‚Äî presence or absence of a feature     | Used for **binary text features** (e.g., word present or not)             |\n",
    "\n",
    "In summary:\n",
    "\n",
    "Gaussian NB ‚Üí continuous data\n",
    "\n",
    "Multinomial NB ‚Üí count data\n",
    "\n",
    "Bernoulli NB ‚Üí binary data\n",
    "\n",
    "Each variant models data differently to match the nature of the input features, improving performance and accuracy.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ff97fe2-2d40-462e-a3f3-675ab86fa3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Gaussian Na√Øve Bayes on Breast Cancer dataset: 0.9737\n"
     ]
    }
   ],
   "source": [
    "#Question 10: Breast Cancer Dataset\n",
    "#Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
    "#Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from sklearn.datasets\n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data      # Features\n",
    "y = data.target    # Target labels\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the Gaussian Na√Øve Bayes classifier\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of Gaussian Na√Øve Bayes on Breast Cancer dataset: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ed44df-0cca-49ad-b6d1-912a9805fdfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad671ead-f15a-4d9c-b528-43532ee5e3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecb56d0-9876-48bc-a87c-a860ea73406c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706183e9-3fc6-4886-bcb8-e6900092c622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f08975-3854-4aac-ae96-b67471360785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a9e482-0067-4b64-aebf-a83611b18539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149810f4-a2f3-49e6-882c-3e139301af98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30389afa-c566-45a7-a5c4-346d5f52ec74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02898d86-8bcc-4236-9af2-891ebbc509bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f37e473-b6cb-4566-9d6c-df805f92ff78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910dc5f5-8073-4cc1-9a9a-020902bb5b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684633b0-1349-4047-a48f-81f5422a06e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223d7192-13b2-4b0b-b965-7c6940c57da8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee43f24d-0aa9-4b48-b8b7-27af612ecdd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266a5e55-99fb-4ed4-8f1e-3dba4390a032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38b5ed7-d5d5-41c9-85e8-7827f74ab1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea59a7-b890-4fa3-9d96-128eeee1daba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d55125-a188-494f-8945-4905e05ded4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fee767-78f1-424e-8a13-acf742ecb1d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb63b326-a182-4948-9ed3-9840b8bc5727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb1f29d-d5f7-4f3c-be56-8298a03f746d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a640a36-004f-405e-85c6-e47243622f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5de23f5-1b1f-44ff-93d6-154aa2f6c9c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
